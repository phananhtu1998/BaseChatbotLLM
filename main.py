from typing import Annotated
import os
import re
from langchain.chat_models import init_chat_model
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START
from langgraph.graph.message import add_messages
from sentence_transformers import SentenceTransformer, CrossEncoder
import numpy as np
from opensearchpy import OpenSearch

# --- TÃªn index OpenSearch ---
index_name = "chatbot_docs"

# --- Káº¿t ná»‘i OpenSearch ---
client = OpenSearch(
    hosts=[{"host": "localhost", "port": 9200}],
    http_auth=("admin", "Thaco@1234"),
    use_ssl=True,
    verify_certs=False,
    ssl_assert_hostname=False,
    ssl_show_warn=False,
)

# --- Load model embedding (pháº£i dÃ¹ng cÃ¹ng model nhÆ° khi táº¡o embeddings) ---
try:
    model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
    print("âœ… Sá»­ dá»¥ng model paraphrase-multilingual-MiniLM-L12-v2")
except:
    try:
        model = SentenceTransformer("all-MiniLM-L12-v2") 
        print("âœ… Sá»­ dá»¥ng model all-MiniLM-L12-v2")
    except:
        model = SentenceTransformer("all-MiniLM-L6-v2")
        print("âœ… Sá»­ dá»¥ng model all-MiniLM-L6-v2")

# --- Load reranker ---
try:
    reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')
except:
    reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

def normalize(v):
    norm = np.linalg.norm(v)
    return v / norm if norm > 0 else v

def preprocess_text(text):
    """Tiá»n xá»­ lÃ½ text Ä‘á»ƒ tÃ¡ch cÃ¡c tá»« dÃ­nh liá»n nhau"""
    text = re.sub(r'([a-zÃ Ã¡áº£Ã£áº¡Ã¢áº§áº¥áº©áº«áº­Ãªá»áº¿á»ƒá»…á»‡Ã´á»“á»‘á»•á»—á»™Æ°á»«á»©á»­á»¯á»±Ä‘])([A-ZÃ€Ãáº¢Ãƒáº Ã‚áº¦áº¤áº¨áºªáº¬ÃŠá»€áº¾á»‚á»„á»†Ã”á»’á»á»”á»–á»˜Æ¯á»ªá»¨á»¬á»®á»°Ä])', r'\1 \2', text)
    text = re.sub(r'([a-zÃ Ã¡áº£Ã£áº¡Ã¢áº§áº¥áº©áº«áº­Ãªá»áº¿á»ƒá»…á»‡Ã´á»“á»‘á»•á»—á»™Æ°á»«á»©á»­á»¯á»±Ä‘])(\d)', r'\1 \2', text)
    text = re.sub(r'(\d)([a-zÃ Ã¡áº£Ã£áº¡Ã¢áº§áº¥áº©áº«áº­Ãªá»áº¿á»ƒá»…á»‡Ã´á»“á»‘á»•á»—á»™Æ°á»«á»©á»­á»¯á»±Ä‘A-ZÃ€Ãáº¢Ãƒáº Ã‚áº¦áº¤áº¨áºªáº¬ÃŠá»€áº¾á»‚á»„á»†Ã”á»’á»á»”á»–á»˜Æ¯á»ªá»¨á»¬á»®á»°Ä])', r'\1 \2', text)
    return text

def search_similar(query_text, top_k=50):
    """TÃ¬m kiáº¿m káº¿t há»£p vector similarity vÃ  text search"""
    
    # Tiá»n xá»­ lÃ½ query
    processed_query = preprocess_text(query_text)
    
    # Táº¡o embedding cho query
    query_embedding = model.encode([processed_query])[0].tolist()
    
    # TÃ¬m kiáº¿m káº¿t há»£p KNN vÃ  text search
    search_query = {
        "size": top_k,
        "query": {
            "bool": {
                "should": [
                    {
                        "knn": {
                            "embedding": {
                                "vector": query_embedding,
                                "k": top_k
                            }
                        }
                    },
                    {
                        "match": {
                            "text": {
                                "query": processed_query,
                                "boost": 0.5
                            }
                        }
                    },
                    {
                        "match_phrase": {
                            "text": {
                                "query": processed_query,
                                "boost": 1.0
                            }
                        }
                    }
                ],
                "minimum_should_match": 1
            }
        },
        "_source": ["text", "chunk_index", "length"]
    }
    
    try:
        response = client.search(index=index_name, body=search_query)
        hits = response["hits"]["hits"]
        
        # Láº¥y text vÃ  tiá»n xá»­ lÃ½
        docs_with_score = []
        for hit in hits:
            text = preprocess_text(hit["_source"]["text"])
            score = hit["_score"]
            docs_with_score.append((text, score))
        
        # Sáº¯p xáº¿p theo score
        docs_with_score.sort(key=lambda x: x[1], reverse=True)
        docs = [doc for doc, score in docs_with_score]
        
        print(f"ğŸ” TÃ¬m tháº¥y {len(docs)} documents")
        return docs
        
    except Exception as e:
        print(f"âŒ Lá»—i search: {e}")
        # Fallback vá» KNN search Ä‘Æ¡n giáº£n
        return search_knn_only(query_text, top_k)

def search_knn_only(query_text, top_k=50):
    """Fallback: chá»‰ sá»­ dá»¥ng KNN search"""
    processed_query = preprocess_text(query_text)
    query_embedding = model.encode([processed_query])[0].tolist()
    
    search_query = {
        "size": top_k,
        "query": {
            "knn": {
                "embedding": {
                    "vector": query_embedding,
                    "k": top_k
                }
            }
        },
        "_source": ["text"]
    }
    
    try:
        response = client.search(index=index_name, body=search_query)
        hits = response["hits"]["hits"]
        docs = [preprocess_text(hit["_source"]["text"]) for hit in hits]
        print(f"ğŸ” KNN Fallback: TÃ¬m tháº¥y {len(docs)} documents")
        return docs
    except Exception as e:
        print(f"âŒ Lá»—i KNN search: {e}")
        return []

def keyword_relevance_score(query, doc):
    """TÃ­nh Ä‘iá»ƒm relevance dá»±a trÃªn tá»« khÃ³a"""
    query_lower = query.lower()
    doc_lower = doc.lower()
    
    score = 0
    
    # TÃ¬m tÃªn ngÆ°á»i
    names = re.findall(r'\b[A-ZÃ€Ãáº¢Ãƒáº Ã‚áº¦áº¤áº¨áºªáº¬ÃŠá»€áº¾á»‚á»„á»†Ã”á»’á»á»”á»–á»˜Æ¯á»ªá»¨á»¬á»®á»°Ä][a-zÃ Ã¡áº£Ã£áº¡Ã¢áº§áº¥áº©áº«áº­Ãªá»áº¿á»ƒá»…á»‡Ã´á»“á»‘á»•á»—á»™Æ°á»«á»©á»­á»¯á»±Ä‘]+(?:\s+[A-ZÃ€Ãáº¢Ãƒáº Ã‚áº¦áº¤áº¨áºªáº¬ÃŠá»€áº¾á»‚á»„á»†Ã”á»’á»á»”á»–á»˜Æ¯á»ªá»¨á»¬á»®á»°Ä][a-zÃ Ã¡áº£Ã£áº¡Ã¢áº§áº¥áº©áº«áº­Ãªá»áº¿á»ƒá»…á»‡Ã´á»“á»‘á»•á»—á»™Æ°á»«á»©á»­á»¯á»±Ä‘]+)*', query)
    for name in names:
        if name.lower() in doc_lower:
            score += 3
    
    # TÃ¬m nÄƒm sinh
    years = re.findall(r'\b(19|20)\d{2}\b', query)
    for year in years:
        if year in doc_lower:
            score += 2
    
    # Tá»« khÃ³a liÃªn quan Ä‘áº¿n sinh nÄƒm
    if any(word in query_lower for word in ["sinh", "nÄƒm", "born"]):
        if any(word in doc_lower for word in ["sinh", "nÄƒm", "born"]):
            score += 1
    
    # CÃ¡c tá»« khÃ³a quan trá»ng khÃ¡c
    important_keywords = ["giÃ¡m Ä‘á»‘c", "chá»§ tá»‹ch", "tá»•ng giÃ¡m Ä‘á»‘c", "phÃ³", "trÆ°á»Ÿng", "Ã´ng", "bÃ "]
    for keyword in important_keywords:
        if keyword in query_lower and keyword in doc_lower:
            score += 1
    
    return score

def rerank_docs(query, docs, top_k=5):
    """Cáº£i thiá»‡n reranking vá»›i káº¿t há»£p keyword matching vÃ  cross-encoder"""
    
    if not docs:
        return []
    
    # BÆ°á»›c 1: Lá»c theo keyword relevance
    keyword_scores = [(doc, keyword_relevance_score(query, doc)) for doc in docs]
    keyword_scores.sort(key=lambda x: x[1], reverse=True)
    
    # Láº¥y top docs cÃ³ keyword relevance cao
    high_keyword_docs = [doc for doc, score in keyword_scores[:15] if score > 0]
    remaining_docs = [doc for doc, score in keyword_scores if score == 0][:10]
    
    # Káº¿t há»£p
    docs_to_rerank = high_keyword_docs + remaining_docs
    
    # BÆ°á»›c 2: Sá»­ dá»¥ng cross-encoder cho reranking
    if len(docs_to_rerank) == 0:
        docs_to_rerank = docs[:15]
    
    try:
        pairs = [[query, doc] for doc in docs_to_rerank]
        cross_scores = reranker.predict(pairs)
        
        # Káº¿t há»£p Ä‘iá»ƒm keyword vÃ  cross-encoder
        final_scores = []
        for i, (doc, cross_score) in enumerate(zip(docs_to_rerank, cross_scores)):
            keyword_score = keyword_relevance_score(query, doc)
            # Trá»ng sá»‘: 70% cross-encoder, 30% keyword
            final_score = 0.7 * cross_score + 0.3 * keyword_score
            final_scores.append((doc, final_score, keyword_score, cross_score))
        
        final_scores.sort(key=lambda x: x[1], reverse=True)
        
        # Debug info
        print(f"ğŸ“Š Reranking scores:")
        for i, (doc, final_score, kw_score, cross_score) in enumerate(final_scores[:5], 1):
            print(f"  {i}. Final: {final_score:.3f} (KW: {kw_score}, Cross: {cross_score:.3f}) - {doc[:80]}...")
        
        return [doc for doc, _, _, _ in final_scores[:top_k]]
    
    except Exception as e:
        print(f"âŒ Reranker error: {e}")
        # Fallback vá» keyword ranking
        return [doc for doc, score in keyword_scores[:top_k]]

# --- Äá»‹nh nghÄ©a State cho langgraph ---
class State(TypedDict):
    messages: Annotated[list, add_messages]

graph_builder = StateGraph(State)

# Cáº§n set API key
os.environ["GOOGLE_API_KEY"] = "AIzaSyDFUKW4QZ0WeQw5_Bz9kbinynstDL8ayL0"
llm = init_chat_model("google_genai:gemini-2.0-flash")

def chatbot(state: State):
    user_message = state["messages"][-1].content
    print(f"ğŸ¤” User query: {user_message}")

    # TÃ¬m kiáº¿m vÄƒn báº£n liÃªn quan
    docs = search_similar(user_message, top_k=30)
    print(f"ğŸ“š Docs found: {len(docs)}")
    
    if not docs:
        return {"messages": [{"role": "assistant", "content": "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan trong dá»¯ liá»‡u."}]}
    
    # Rerank vá»›i thuáº­t toÃ¡n cáº£i thiá»‡n
    reranked_docs = rerank_docs(user_message, docs, top_k=10)
    print(f"ğŸ† Top {len(reranked_docs)} reranked docs selected")
    
    # Láº¥y top 5 tÃ i liá»‡u Ä‘á»ƒ Ä‘Æ°a vÃ o prompt
    selected_docs = reranked_docs[:5]

    context = "\n---\n".join(selected_docs)
    prompt = f"""
    Dá»±a vÃ o cÃ¡c tÃ i liá»‡u sau (má»—i dÃ²ng lÃ  má»™t Ä‘oáº¡n thÃ´ng tin, cÃ³ thá»ƒ láº·p, giá»¯ nguyÃªn thÃ´ng tin thá»i gian, Ä‘á»‹a danh, tÃªn tá»• chá»©c nhÆ° trong tÃ i liá»‡u):

    {context}

    Há»i: {user_message}

    Tráº£ lá»i chÃ­nh xÃ¡c vÃ  sÃºc tÃ­ch, bao gá»“m Ä‘áº§y Ä‘á»§ thÃ´ng tin thá»i gian (bao gá»“m nÄƒm náº¿u cÃ³ trong tÃ i liá»‡u), loáº¡i bá» cÃ¡c kÃ½ tá»± Ä‘áº·c biá»‡t (nhÆ° gáº¡ch dÆ°á»›i) trong tÃªn ngÆ°á»i hoáº·c tá»• chá»©c Ä‘á»ƒ Ä‘áº£m báº£o cÃ¢u tráº£ lá»i tá»± nhiÃªn vÃ  Ä‘Ãºng ngá»¯ phÃ¡p tiáº¿ng Viá»‡t, khÃ´ng suy Ä‘oÃ¡n tá»« viá»‡c dá»¯ liá»‡u cÃ³ thá»ƒ láº·p láº¡i. Náº¿u thÃ´ng tin Ä‘Æ°á»£c há»i khÃ´ng cÃ³ trong tÃ i liá»‡u, tráº£ lá»i: "Dá»¯ liá»‡u nÃ y mÃ¬nh chÆ°a cÃ³ thÃ´ng tin."
    """

    try:
        response = llm.invoke([{"role": "user", "content": prompt}])
        return {"messages": [response]}
    except Exception as e:
        print(f"âŒ LLM error: {e}")
        return {"messages": [{"role": "assistant", "content": "Xin lá»—i, Ä‘Ã£ xáº£y ra lá»—i khi xá»­ lÃ½ cÃ¢u há»i cá»§a báº¡n."}]}

graph_builder.add_node("chatbot", chatbot)
graph_builder.add_edge(START, "chatbot")
graph = graph_builder.compile()

def test_search():
    """HÃ m test Ä‘á»ƒ kiá»ƒm tra search function"""
    print("ğŸ§ª Testing search function...")
    
    test_queries = [
        "Tráº§n BÃ¡ DÆ°Æ¡ng",
        "sinh nÄƒm 1960",
        "giÃ¡m Ä‘á»‘c Ä‘iá»u hÃ nh"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” Test query: '{query}'")
        docs = search_similar(query, top_k=5)
        if docs:
            print(f"âœ… Found {len(docs)} docs")
            for i, doc in enumerate(docs[:2], 1):
                print(f"  {i}. {doc[:100]}...")
        else:
            print("âŒ No docs found")

def chat_loop():
    print("ğŸ¤– Chat started! Type 'quit', 'exit', 'q', or 'test' to test search.")
    
    while True:
        try:
            user_input = input("\nğŸ‘¤ User: ").strip()
            
            if user_input.lower() in ["quit", "exit", "q"]:
                print("ğŸ‘‹ Goodbye!")
                break
            
            if user_input.lower() == "test":
                test_search()
                continue
            
            if not user_input:
                continue
            
            print("ğŸ¤– Assistant: ", end="", flush=True)
            for event in graph.stream({"messages": [{"role": "user", "content": user_input}]}):
                for value in event.values():
                    if "messages" in value and value["messages"]:
                        print(value["messages"][-1].content)
                        
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Goodbye!")
            break
        except Exception as e:
            print(f"âŒ An error occurred: {str(e)}")

if __name__ == "__main__":
    # Kiá»ƒm tra káº¿t ná»‘i OpenSearch
    try:
        info = client.info()
        print(f"âœ… Connected to OpenSearch: {info['version']['number']}")
        
        # Kiá»ƒm tra index cÃ³ tá»“n táº¡i khÃ´ng
        if client.indices.exists(index=index_name):
            count = client.count(index=index_name)
            print(f"âœ… Index '{index_name}' exists with {count['count']} documents")
        else:
            print(f"âŒ Index '{index_name}' does not exist. Please run the embedding creation script first.")
            exit(1)
            
    except Exception as e:
        print(f"âŒ Cannot connect to OpenSearch: {e}")
        exit(1)
    
    chat_loop()