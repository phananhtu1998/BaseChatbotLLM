from opensearchpy import OpenSearch
from sentence_transformers import SentenceTransformer
from spacy.lang.vi import Vietnamese
import uuid
import json
import re
from typing import List, Dict
import hashlib

# Káº¿t ná»‘i OpenSearch
client = OpenSearch(
    hosts=[{"host": "localhost", "port": 9200}],
    http_compress=True,
    http_auth=("admin", "Thaco@1234"),
    use_ssl=True,
    verify_certs=False,
    ssl_assert_hostname=False,
    ssl_show_warn=False,
)

index_name = "chatbot_docs"

# Cáº¥u hÃ¬nh splitting
MIN_CHUNK_SIZE = 50      
MAX_CHUNK_SIZE = 300     
OVERLAP_SIZE = 50        
MIN_SENTENCE_LENGTH = 10 

def preprocess_text(text: str) -> str:
    """Tiá»n xá»­ lÃ½ text Ä‘á»ƒ cáº£i thiá»‡n cháº¥t lÆ°á»£ng"""
    # Loáº¡i bá» kÃ½ tá»± khÃ´ng cáº§n thiáº¿t
    text = re.sub(r'\s+', ' ', text)  # Chuáº©n hÃ³a khoáº£ng tráº¯ng
    
    # TÃ¡ch cÃ¡c tá»« dÃ­nh liá»n (nhÆ° sinhnÄƒm1960 -> sinh nÄƒm 1960)
    text = re.sub(r'([a-zÃ Ã¡áº£Ã£áº¡Ã¢áº§áº¥áº©áº«áº­Ãªá»áº¿á»ƒá»…á»‡Ã´á»“á»‘á»•á»—á»™Æ°á»«á»©á»­á»¯á»±Ä‘])([A-ZÃ€Ãáº¢Ãƒáº Ã‚áº¦áº¤áº¨áºªáº¬ÃŠá»€áº¾á»‚á»„á»†Ã”á»’á»á»”á»–á»˜Æ¯á»ªá»¨á»¬á»®á»°Ä])', r'\1 \2', text)
    text = re.sub(r'([a-zÃ Ã¡áº£Ã£áº¡Ã¢áº§áº¥áº©áº«áº­Ãªá»áº¿á»ƒá»…á»‡Ã´á»“á»‘á»•á»—á»™Æ°á»«á»©á»­á»¯á»±Ä‘])(\d)', r'\1 \2', text)
    text = re.sub(r'(\d)([a-zÃ Ã¡áº£Ã£áº¡Ã¢áº§áº¥áº©áº«áº­Ãªá»áº¿á»ƒá»…á»‡Ã´á»“á»‘á»•á»—á»™Æ°á»«á»©á»­á»¯á»±Ä‘])', r'\1 \2', text)
    
    return text.strip()

def smart_text_split(text: str) -> List[str]:
    """Chia text thÃ nh chunks thÃ´ng minh vá»›i overlap"""
    
    # Tiá»n xá»­ lÃ½ text
    processed_text = preprocess_text(text)
    
    # Khá»Ÿi táº¡o spaCy
    nlp = Vietnamese()
    nlp.add_pipe("sentencizer")
    
    # TÃ¡ch cÃ¢u
    doc = nlp(processed_text)
    sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) >= MIN_SENTENCE_LENGTH]
    
    print(f"ğŸ“Š ÄÃ£ tÃ¡ch Ä‘Æ°á»£c {len(sentences)} cÃ¢u")
    
    # Táº¡o chunks vá»›i overlap
    chunks = []
    current_chunk = ""
    current_sentences = []
    
    for i, sentence in enumerate(sentences):
        # Kiá»ƒm tra xem cÃ³ nÃªn táº¡o chunk má»›i khÃ´ng
        if (len(current_chunk) + len(sentence) > MAX_CHUNK_SIZE and 
            len(current_chunk) >= MIN_CHUNK_SIZE):
            
            # LÆ°u chunk hiá»‡n táº¡i
            if current_chunk.strip():
                chunks.append(current_chunk.strip())
            
            # Táº¡o overlap: giá»¯ láº¡i 1-2 cÃ¢u cuá»‘i
            overlap_sentences = current_sentences[-1:] if current_sentences else []
            overlap_text = " ".join(overlap_sentences)
            
            if overlap_text:
                current_chunk = overlap_text + " " + sentence
                current_sentences = overlap_sentences + [sentence]
            else:
                current_chunk = sentence
                current_sentences = [sentence]
        else:
            # ThÃªm cÃ¢u vÃ o chunk hiá»‡n táº¡i
            if current_chunk:
                current_chunk += " " + sentence
            else:
                current_chunk = sentence
            current_sentences.append(sentence)
    
    # ThÃªm chunk cuá»‘i cÃ¹ng
    if current_chunk.strip() and len(current_chunk.strip()) >= MIN_CHUNK_SIZE:
        chunks.append(current_chunk.strip())
    
    # Lá»c chunks quÃ¡ ngáº¯n
    chunks = [chunk for chunk in chunks if len(chunk) >= MIN_CHUNK_SIZE]
    
    print(f"ğŸ“¦ Táº¡o Ä‘Æ°á»£c {len(chunks)} chunks")
    print(f"ğŸ“ Äá»™ dÃ i chunk: min={min(len(c) for c in chunks)}, max={max(len(c) for c in chunks)}, avg={sum(len(c) for c in chunks)/len(chunks):.1f}")
    
    return chunks

def create_embeddings():
    """Táº¡o embeddings Ä‘Æ¡n giáº£n vÃ  tÆ°Æ¡ng thÃ­ch"""
    print("ğŸš€ Báº¯t Ä‘áº§u táº¡o embeddings...")
    
    # Load model
    try:
        model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
        print("âœ… Sá»­ dá»¥ng model paraphrase-multilingual-MiniLM-L12-v2")
    except:
        try:
            model = SentenceTransformer("all-MiniLM-L12-v2") 
            print("âœ… Sá»­ dá»¥ng model all-MiniLM-L12-v2")
        except:
            model = SentenceTransformer("all-MiniLM-L6-v2")
            print("âœ… Sá»­ dá»¥ng model all-MiniLM-L6-v2")
    
    # Äá»c dá»¯ liá»‡u
    print("ğŸ“– Äá»c file...")
    with open("data.txt", "r", encoding="utf-8") as f:
        raw_text = f.read()
    
    print(f"ğŸ“„ File gá»‘c: {len(raw_text):,} kÃ½ tá»±")
    
    # Chia text thÃ nh chunks
    chunks = smart_text_split(raw_text)
    
    if not chunks:
        print("âŒ KhÃ´ng cÃ³ chunks nÃ o Ä‘Æ°á»£c táº¡o!")
        return
    
    # Táº¡o embeddings
    print("ğŸ”¢ Táº¡o embeddings...")
    embeddings = model.encode(chunks, show_progress_bar=True)
    print(f"âœ… Táº¡o Ä‘Æ°á»£c {len(embeddings)} embeddings, dimension: {len(embeddings[0])}")
    
    # Táº¡o index OpenSearch Ä‘Æ¡n giáº£n
    create_simple_index(len(embeddings[0]))
    
    # Chuáº©n bá»‹ documents
    print("ğŸ“¦ Chuáº©n bá»‹ documents...")
    documents = []
    
    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
        doc_id = f"chunk_{i}_{hashlib.md5(chunk.encode()).hexdigest()[:8]}"
        
        document = {
            "_index": index_name,
            "_id": doc_id,
            "_source": {
                "id": doc_id,
                "text": chunk,
                "embedding": embedding.tolist(),
                "chunk_index": i,
                "length": len(chunk)
            }
        }
        documents.append(document)
    
    # Insert vÃ o OpenSearch
    print("ğŸ’¾ ChÃ¨n dá»¯ liá»‡u...")
    from opensearchpy.helpers import bulk
    
    try:
        success, failed = bulk(client, documents, chunk_size=50)
        print(f"âœ… ThÃ nh cÃ´ng: {success} documents")
        if failed:
            print(f"âŒ Tháº¥t báº¡i: {len(failed)} documents")
    except Exception as e:
        print(f"âŒ Lá»—i insert: {e}")
        return
    
    print("ğŸ‰ HoÃ n thÃ nh!")
    print(f"ğŸ“Š Thá»‘ng kÃª cuá»‘i:")
    print(f"   - Chunks: {len(chunks)}")
    print(f"   - Embedding dimension: {len(embeddings[0])}")
    print(f"   - Äá»™ dÃ i trung bÃ¬nh: {sum(len(c) for c in chunks) / len(chunks):.1f} kÃ½ tá»±")

def create_simple_index(embedding_dim):
    """Táº¡o index OpenSearch Ä‘Æ¡n giáº£n"""
    
    # XÃ³a index cÅ© náº¿u tá»“n táº¡i
    if client.indices.exists(index=index_name):
        print(f"ğŸ—‘ï¸ XÃ³a index cÅ©...")
        client.indices.delete(index=index_name)
    
    # Táº¡o index má»›i vá»›i cáº¥u hÃ¬nh tá»‘i thiá»ƒu
    index_body = {
        "settings": {
            "index": {
                "number_of_shards": 1,
                "number_of_replicas": 0,
                "knn": True
            }
        },
        "mappings": {
            "properties": {
                "id": {"type": "keyword"},
                "text": {"type": "text"},
                "embedding": {
                    "type": "knn_vector",
                    "dimension": embedding_dim
                },
                "chunk_index": {"type": "integer"},
                "length": {"type": "integer"}
            }
        }
    }
    
    try:
        client.indices.create(index=index_name, body=index_body)
        print(f"âœ… Táº¡o index '{index_name}' thÃ nh cÃ´ng")
    except Exception as e:
        print(f"âŒ Lá»—i táº¡o index: {e}")
        raise

if __name__ == "__main__":
    create_embeddings()