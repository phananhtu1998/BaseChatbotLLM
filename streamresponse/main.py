# main.py - Backend FastAPI v·ªõi Gemini 2.0 Flash
# requirements.txt:
# fastapi==0.104.1
# uvicorn==0.24.0
# google-generativeai==0.3.2
# python-multipart==0.0.6
# pydantic==2.5.0

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from typing import List, Optional
import google.generativeai as genai
import json
import asyncio
import os
from datetime import datetime
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Pydantic models
class ChatMessage(BaseModel):
    role: str  # 'user' ho·∫∑c 'assistant'
    content: str
    timestamp: Optional[datetime] = None

class ChatRequest(BaseModel):
    message: str
    history: Optional[List[ChatMessage]] = []
    model: Optional[str] = "gemini-2.0-flash-exp"
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 2048

class ChatResponse(BaseModel):
    response: str
    model: str
    timestamp: datetime

# FastAPI app
app = FastAPI(
    title="Gemini 2.0 Flash Chatbot API",
    description="API cho chatbot s·ª≠ d·ª•ng Gemini 2.0 Flash v·ªõi streaming response",
    version="1.0.0"
)

# CORS middleware - cho ph√©p frontend k·∫øt n·ªëi
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Trong production n√™n ch·ªâ ƒë·ªãnh c·ª• th·ªÉ domain
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mount static files (ƒë·ªÉ serve HTML)
app.mount("/static", StaticFiles(directory="static"), name="static")

# Kh·ªüi t·∫°o Gemini
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    logger.warning("‚ö†Ô∏è  GEMINI_API_KEY ch∆∞a ƒë∆∞·ª£c set. H√£y set trong environment variables.")
    logger.info("üí° C√°ch set: export GEMINI_API_KEY='your-api-key'")

if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

class GeminiChatbot:
    def __init__(self):
        self.model_name = "gemini-2.0-flash-exp"
        self.generation_config = {
            "temperature": 0.7,
            "top_p": 0.8,
            "top_k": 40,
            "max_output_tokens": 2048,
        }
        self.safety_settings = [
            {
                "category": "HARM_CATEGORY_HARASSMENT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_HATE_SPEECH",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            }
        ]

    def get_model(self):
        """T·∫°o model Gemini v·ªõi c·∫•u h√¨nh"""
        return genai.GenerativeModel(
            model_name=self.model_name,
            generation_config=self.generation_config,
            safety_settings=self.safety_settings
        )

    async def stream_response(self, message: str, history: List[ChatMessage] = None):
        """Stream response t·ª´ Gemini v·ªõi real-time chunks"""
        try:
            model = self.get_model()
            
            # Chuy·ªÉn ƒë·ªïi history sang format Gemini
            gemini_history = []
            if history:
                for msg in history[-10]:  # Gi·ªØ 10 tin nh·∫Øn g·∫ßn nh·∫•t ƒë·ªÉ tr√°nh context qu√° d√†i
                    role = "user" if msg.role == "user" else "model"
                    gemini_history.append({
                        "role": role,
                        "parts": [{"text": msg.content}]
                    })

            # T·∫°o chat session v·ªõi history
            chat = model.start_chat(history=gemini_history)
            
            # G·ªçi Gemini API v·ªõi streaming
            response = await asyncio.to_thread(
                chat.send_message, 
                message, 
                stream=True
            )
            
            full_response = ""
            chunk_count = 0
            
            # Stream t·ª´ng chunk
            for chunk in response:
                if chunk.text:
                    chunk_text = chunk.text
                    full_response += chunk_text
                    chunk_count += 1
                    
                    # G·ª≠i chunk data qua Server-Sent Events
                    yield f"data: {json.dumps({
                        'type': 'chunk',
                        'content': chunk_text,
                        'full_content': full_response,
                        'chunk_id': chunk_count,
                        'timestamp': datetime.now().isoformat()
                    }, ensure_ascii=False)}\n\n"
                    
                    # Th√™m delay nh·ªè ƒë·ªÉ t·∫°o hi·ªáu ·ª©ng streaming t·ª± nhi√™n
                    await asyncio.sleep(0.02)
            
            # G·ª≠i signal ho√†n th√†nh
            yield f"data: {json.dumps({
                'type': 'done',
                'content': '',
                'full_content': full_response,
                'total_chunks': chunk_count,
                'timestamp': datetime.now().isoformat()
            }, ensure_ascii=False)}\n\n"
            
            logger.info(f"‚úÖ Stream completed: {chunk_count} chunks, {len(full_response)} chars")
            
        except Exception as e:
            logger.error(f"‚ùå Streaming error: {str(e)}")
            yield f"data: {json.dumps({
                'type': 'error',
                'error': f'L·ªói khi k·∫øt n·ªëi v·ªõi Gemini: {str(e)}',
                'timestamp': datetime.now().isoformat()
            }, ensure_ascii=False)}\n\n"

    async def get_response(self, message: str, history: List[ChatMessage] = None):
        """Non-streaming response (backup method)"""
        try:
            model = self.get_model()
            
            # Chuy·ªÉn ƒë·ªïi history
            gemini_history = []
            if history:
                for msg in history[-10]:
                    role = "user" if msg.role == "user" else "model"
                    gemini_history.append({
                        "role": role,
                        "parts": [{"text": msg.content}]
                    })

            chat = model.start_chat(history=gemini_history)
            response = await asyncio.to_thread(chat.send_message, message)
            
            return response.text
            
        except Exception as e:
            logger.error(f"‚ùå Response error: {str(e)}")
            raise HTTPException(status_code=500, detail=f"L·ªói Gemini API: {str(e)}")

# Kh·ªüi t·∫°o chatbot instance
chatbot = GeminiChatbot()

@app.get("/")
async def root():
    """Root endpoint - h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng"""
    return {
        "message": "ü§ñ Gemini 2.0 Flash Chatbot API",
        "version": "1.0.0",
        "endpoints": {
            "stream_chat": "POST /api/chat/stream",
            "regular_chat": "POST /api/chat", 
            "health": "GET /api/health",
            "docs": "GET /docs"
        },
        "frontend": "/static/index.html",
        "status": "‚úÖ API ƒëang ho·∫°t ƒë·ªông"
    }

@app.post("/api/chat/stream")
async def stream_chat(request: ChatRequest):
    """
    Stream chat endpoint - tr·∫£ v·ªÅ response t·ª´ng chunk
    
    Request body:
    - message: Tin nh·∫Øn ng∆∞·ªùi d√πng
    - history: L·ªãch s·ª≠ chat (optional)
    - model: Model name (optional, default: gemini-2.0-flash-exp)
    - temperature: ƒê·ªô s√°ng t·∫°o 0-1 (optional, default: 0.7)
    """
    if not GEMINI_API_KEY:
        raise HTTPException(
            status_code=500, 
            detail="‚ùå GEMINI_API_KEY ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh. Vui l√≤ng set environment variable."
        )
    
    logger.info(f"üöÄ Starting stream for message: {request.message[:50]}...")
    
    return StreamingResponse(
        chatbot.stream_response(request.message, request.history),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "X-Accel-Buffering": "no",  # T·∫Øt buffering cho nginx
        }
    )

@app.post("/api/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Regular chat endpoint - tr·∫£ v·ªÅ response m·ªôt l·∫ßn
    """
    if not GEMINI_API_KEY:
        raise HTTPException(
            status_code=500, 
            detail="‚ùå GEMINI_API_KEY ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh"
        )
    
    logger.info(f"üí¨ Processing chat: {request.message[:50]}...")
    
    response = await chatbot.get_response(request.message, request.history)
    
    return ChatResponse(
        response=response,
        model=request.model or "gemini-2.0-flash-exp",
        timestamp=datetime.now()
    )

@app.get("/api/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy" if GEMINI_API_KEY else "warning",
        "timestamp": datetime.now().isoformat(),
        "model": "gemini-2.0-flash-exp",
        "api_configured": bool(GEMINI_API_KEY),
        "message": "‚úÖ API ƒëang ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng" if GEMINI_API_KEY else "‚ö†Ô∏è Ch∆∞a c·∫•u h√¨nh GEMINI_API_KEY"
    }

@app.get("/api/models")
async def get_available_models():
    """Danh s√°ch models c√≥ s·∫µn"""
    return {
        "available_models": [
            "gemini-2.0-flash-exp",
            "gemini-1.5-pro",
            "gemini-1.5-flash"
        ],
        "current_model": chatbot.model_name,
        "generation_config": chatbot.generation_config
    }

# Error handlers
@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    return {
        "error": True,
        "status_code": exc.status_code,
        "message": exc.detail,
        "timestamp": datetime.now().isoformat()
    }

@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    logger.error(f"‚ùå Unhandled error: {str(exc)}")
    return {
        "error": True,
        "status_code": 500,
        "message": "ƒê√£ x·∫£y ra l·ªói kh√¥ng mong mu·ªën",
        "timestamp": datetime.now().isoformat()
    }

if __name__ == "__main__":
    import uvicorn
    
    print("\nüöÄ Starting Gemini Chatbot Server...")
    print("üìã Th√¥ng tin:")
    print(f"   ‚Ä¢ Model: {chatbot.model_name}")
    print(f"   ‚Ä¢ API Key: {'‚úÖ ƒê√£ c·∫•u h√¨nh' if GEMINI_API_KEY else '‚ùå Ch∆∞a c·∫•u h√¨nh'}")
    print(f"   ‚Ä¢ Server: http://localhost:8000")
    print(f"   ‚Ä¢ API Docs: http://localhost:8000/docs")
    print(f"   ‚Ä¢ Frontend: http://localhost:8000/static/index.html")
    print("\nüí° C√°ch set API key: export GEMINI_API_KEY='your-api-key'\n")
    
    uvicorn.run(
        "main:app", 
        host="0.0.0.0", 
        port=8000, 
        reload=True,
        log_level="info"
    )