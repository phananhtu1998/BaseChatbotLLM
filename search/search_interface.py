import re
import time
import random
from typing import List
from datetime import datetime
from urllib.parse import quote_plus
import requests
from bs4 import BeautifulSoup
from .models import SearchResult, RankedResult
from .reranker import ContentReranker
from .gemini_api import GeminiAPI

class SearchInterface:
    def __init__(self, api_key: str, model: str = "gemini-2.0-flash"):
        self.reranker = ContentReranker()
        self.search_history = []
        self.gemini_api = GeminiAPI(api_key, model)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        self.cache = {}
        # ThÃªm thÃ´ng tin thá»i gian
        self.current_date = datetime.now().strftime("%d/%m/%Y")
        self.llm_training_cutoff = "thÃ¡ng 6/2024"  # Thá»i Ä‘iá»ƒm model Ä‘Æ°á»£c train Ä‘áº¿n

    def search_google(self, query: str, max_results: int = 10) -> List[SearchResult]:
        """TÃ¬m kiáº¿m vá»›i Google (cáº£i thiá»‡n compatibility)."""
        retries = 3
        backoff_factor = 5
        
        for attempt in range(retries):
            try:
                # ThÃªm delay ngáº«u nhiÃªn Ä‘á»ƒ trÃ¡nh bá»‹ block
                time.sleep(random.uniform(3, 6))
                
                # Encode query cho URL vá»›i cÃ¡c parameter tá»‘i Æ°u
                encoded_query = quote_plus(query)
                search_url = f"https://www.google.com/search?q={encoded_query}&num={max_results}&hl=en&lr=lang_en&safe=off"
                
                print(f"ğŸŒ Äang tÃ¬m kiáº¿m trÃªn Google: {query}")
                
                # Headers cáº£i thiá»‡n Ä‘á»ƒ giáº£ láº­p browser tháº­t
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                    'Sec-Fetch-Dest': 'document',
                    'Sec-Fetch-Mode': 'navigate',
                    'Sec-Fetch-Site': 'none',
                    'Cache-Control': 'max-age=0',
                }
                
                # Gá»­i yÃªu cáº§u
                response = requests.get(search_url, headers=headers, timeout=20)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                results = []
                
                # Multiple selectors cho Google search results
                search_containers = []
                
                # Thá»­ cÃ¡c selector khÃ¡c nhau
                selectors_to_try = [
                    'div.g',
                    'div.tF2Cxc',
                    'div[data-ved]',
                    '.g',
                    '.tF2Cxc'
                ]
                
                for selector in selectors_to_try:
                    containers = soup.select(selector)
                    if containers:
                        search_containers = containers
                        break
                
                if not search_containers:
                    print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y container Google search results")
                    # Fallback: tÃ¬m táº¥t cáº£ div cÃ³ chá»©a link
                    search_containers = soup.find_all('div', string=lambda text: text and 'http' in str(text))
                
                for container in search_containers[:max_results]:
                    try:
                        # Multiple strategies Ä‘á»ƒ láº¥y title vÃ  URL
                        title = ""
                        url = ""
                        description = ""
                        
                        # Strategy 1: TÃ¬m h3 tag
                        h3_elem = container.find('h3')
                        if h3_elem:
                            title = h3_elem.get_text().strip()
                            # TÃ¬m parent link
                            link_elem = h3_elem.find_parent('a')
                            if link_elem:
                                url = link_elem.get('href', '')
                        
                        # Strategy 2: TÃ¬m link Ä‘áº§u tiÃªn trong container
                        if not url:
                            link_elem = container.find('a', href=True)
                            if link_elem:
                                url = link_elem.get('href', '')
                                if not title and link_elem.get_text().strip():
                                    title = link_elem.get_text().strip()
                        
                        # Clean URL
                        if url:
                            if url.startswith('/url?q='):
                                url = url.split('/url?q=')[1].split('&')[0]
                            elif url.startswith('/search?') or url.startswith('#'):
                                continue  # Skip internal Google links
                        
                        # Validate URL
                        if not url or not url.startswith('http'):
                            continue
                        
                        # Láº¥y description tá»« nhiá»u nguá»“n
                        desc_selectors = [
                            '.VwiC3b', '.s3v9rd', '.st', 'span[style*="-webkit-line-clamp"]',
                            '.IsZvec', '.aCOpRe', '.BNeawe', 'div[data-content-feature="1"]'
                        ]
                        
                        for selector in desc_selectors:
                            desc_elem = container.select_one(selector)
                            if desc_elem:
                                description = desc_elem.get_text().strip()
                                break
                        
                        # Fallback description
                        if not description:
                            text_content = container.get_text().strip()
                            if len(text_content) > len(title) + 50:
                                description = text_content[:200] + "..."
                        
                        if title and url:
                            # Scrape ná»™i dung tá»« URL
                            content = self.scrape_content(url)
                            
                            search_result = SearchResult(
                                title=title,
                                url=url,
                                description=description,
                                content=content,
                                source="Google"
                            )
                            results.append(search_result)
                            
                            print(f"âœ… Google - TÃ¬m tháº¥y: {title}")
                    
                    except Exception as e:
                        print(f"âš ï¸ Lá»—i parse Google result: {e}")
                        continue
                
                print(f"ğŸ” Google hoÃ n thÃ nh: {len(results)} káº¿t quáº£")
                return results
            
            except requests.exceptions.HTTPError as e:
                if hasattr(e.response, 'status_code') and e.response.status_code == 429:
                    print(f"âš ï¸ Google 429: QuÃ¡ nhiá»u yÃªu cáº§u. Thá»­ láº¡i sau {backoff_factor * (2 ** attempt)} giÃ¢y...")
                    time.sleep(backoff_factor * (2 ** attempt))
                else:
                    print(f"âŒ Lá»—i Google HTTP: {e}")
                    if attempt == retries - 1:
                        return []
            except Exception as e:
                print(f"âŒ Lá»—i Google search (attempt {attempt + 1}): {e}")
                if attempt == retries - 1:
                    return []
                time.sleep(2)
        
        print("âŒ Google search tháº¥t báº¡i sau nhiá»u láº§n thá»­.")
        return []

    def detect_time_sensitive_query(self, query: str) -> bool:
        """PhÃ¡t hiá»‡n cÃ¢u há»i liÃªn quan Ä‘áº¿n thá»i gian hiá»‡n táº¡i."""
        time_keywords = [
            'tuá»•i', 'age', 'nÄƒm nay', 'hiá»‡n táº¡i', 'bÃ¢y giá»', 'hÃ´m nay', 'today', 'now', 'current',
            'má»›i nháº¥t', 'latest', 'recent', 'gáº§n Ä‘Ã¢y', '2024', '2025', 'this year',
            'tin tá»©c', 'news', 'cáº­p nháº­t', 'update', 'thá»i sá»±'
        ]
        
        query_lower = query.lower()
        return any(keyword in query_lower for keyword in time_keywords)

    def add_time_context_to_prompt(self, original_prompt: str, query: str) -> str:
        """ThÃªm context thá»i gian vÃ o prompt cho LLM."""
        if self.detect_time_sensitive_query(query):
            time_context = f"""
            QUAN TRá»ŒNG - THÃ”NG TIN THá»œI GIAN:
            - NgÃ y hiá»‡n táº¡i: {self.current_date} (30/5/2025)
            - Model Ä‘Æ°á»£c train Ä‘áº¿n: {self.llm_training_cutoff} (thÃ¡ng 6/2024)
            - Khi tráº£ lá»i cÃ¢u há»i vá» tuá»•i, thá»i gian, sá»± kiá»‡n hiá»‡n táº¡i, hÃ£y sá»­ dá»¥ng thÃ´ng tin tá»« search results Ä‘á»ƒ cáº­p nháº­t Ä‘áº¿n thá»i Ä‘iá»ƒm hiá»‡n táº¡i (2025)
            - Náº¿u khÃ´ng cÃ³ thÃ´ng tin má»›i tá»« search, hÃ£y nÃ³i rÃµ ráº±ng thÃ´ng tin cÃ³ thá»ƒ Ä‘Ã£ lá»—i thá»i

            """
            return time_context + original_prompt
        
        return original_prompt

    def search_duckduckgo(self, query: str, max_results: int = 10) -> List[SearchResult]:
        """TÃ¬m kiáº¿m thá»±c táº¿ vá»›i DuckDuckGo (web scraping)."""
        retries = 3
        backoff_factor = 5
        
        for attempt in range(retries):
            try:
                # ThÃªm delay ngáº«u nhiÃªn Ä‘á»ƒ trÃ¡nh bá»‹ block
                time.sleep(random.uniform(2, 5))
                
                # Encode query cho URL
                encoded_query = quote_plus(query)
                search_url = f"https://duckduckgo.com/html/?q={encoded_query}"
                
                print(f"ğŸŒ Äang tÃ¬m kiáº¿m trÃªn DuckDuckGo: {query}")
                
                # Gá»­i yÃªu cáº§u vá»›i cÃ¡c tham sá»‘ phÃ¹ há»£p
                response = self.session.get(search_url, timeout=15)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                results = []
                
                # TÃ¬m cÃ¡c káº¿t quáº£ tÃ¬m kiáº¿m DuckDuckGo
                search_containers = soup.find_all('div', class_='result__body')
                
                for container in search_containers[:max_results]:
                    try:
                        # Láº¥y title
                        title_elem = container.find('a', class_='result__a')
                        if not title_elem:
                            continue
                        title = title_elem.get_text().strip()
                        
                        # Láº¥y URL
                        url = title_elem.get('href')
                        if not url or not url.startswith('http'):
                            continue
                        
                        # Láº¥y description (snippet)
                        desc_elem = container.find('a', class_='result__snippet')
                        description = desc_elem.get_text().strip() if desc_elem else ""
                        
                        if not description:
                            # Fallback: tÃ¬m mÃ´ táº£ trong tháº» khÃ¡c
                            desc_elem = container.find('div', class_='result__snippet')
                            if desc_elem:
                                description = desc_elem.get_text().strip()
                        
                        if title and url:
                            # Scrape ná»™i dung tá»« URL
                            content = self.scrape_content(url)
                            
                            search_result = SearchResult(
                                title=title,
                                url=url,
                                description=description,
                                content=content,
                                source="DuckDuckGo"
                            )
                            results.append(search_result)
                            
                            print(f"âœ… DuckDuckGo - TÃ¬m tháº¥y: {title}")
                    
                    except Exception as e:
                        print(f"âš ï¸ Lá»—i parse DuckDuckGo result: {e}")
                        continue
                
                print(f"ğŸ” DuckDuckGo hoÃ n thÃ nh: {len(results)} káº¿t quáº£")
                return results
            
            except requests.exceptions.HTTPError as e:
                if response.status_code == 429:
                    print(f"âš ï¸ DuckDuckGo 429: QuÃ¡ nhiá»u yÃªu cáº§u. Thá»­ láº¡i sau {backoff_factor * (2 ** attempt)} giÃ¢y...")
                    time.sleep(backoff_factor * (2 ** attempt))
                else:
                    print(f"âŒ Lá»—i DuckDuckGo search: {e}")
                    return []
            except Exception as e:
                print(f"âŒ Lá»—i DuckDuckGo search: {e}")
                return []
    
        print("âŒ DuckDuckGo search tháº¥t báº¡i sau nhiá»u láº§n thá»­.")
        return []

    def search_combined(self, query: str, total_results: int = 10) -> List[SearchResult]:
        """Káº¿t há»£p tÃ¬m kiáº¿m tá»« Google vÃ  DuckDuckGo."""
        print(f"\nğŸ” Báº¯t Ä‘áº§u tÃ¬m kiáº¿m káº¿t há»£p: '{query}'")
        print("ğŸ“Š Chiáº¿n lÆ°á»£c: Google (5 káº¿t quáº£) + DuckDuckGo (5 káº¿t quáº£)")
        
        # Chia Ä‘á»u káº¿t quáº£ giá»¯a 2 search engine
        results_per_engine = total_results // 2
        
        all_results = []
        
        # TÃ¬m kiáº¿m song song (cÃ³ thá»ƒ tá»‘i Æ°u vá»›i threading sau)
        google_results = self.search_google(query, max_results=results_per_engine)
        duckduckgo_results = self.search_duckduckgo(query, max_results=results_per_engine)
        
        # Káº¿t há»£p káº¿t quáº£
        all_results.extend(google_results)
        all_results.extend(duckduckgo_results)
        
        # Loáº¡i bá» duplicate URLs
        seen_urls = set()
        unique_results = []
        
        for result in all_results:
            if result.url not in seen_urls:
                seen_urls.add(result.url)
                unique_results.append(result)
        
        print(f"ğŸ¯ Tá»•ng káº¿t: {len(unique_results)} káº¿t quáº£ unique tá»« {len(all_results)} káº¿t quáº£ gá»‘c")
        
        return unique_results[:total_results]  # Giá»›i háº¡n theo yÃªu cáº§u

    def scrape_content(self, url: str, max_length: int = 1500) -> str:
        """Scrape ná»™i dung tá»« URL."""
        try:
            # ThÃªm delay nhá»
            time.sleep(random.uniform(0.5, 1.5))
            
            response = self.session.get(url, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Loáº¡i bá» script, style, nav, footer
                for tag in soup(["script", "style", "nav", "footer", "header", "aside"]):
                    tag.decompose()
                
                # TÃ¬m ná»™i dung chÃ­nh
                content_selectors = [
                    'article', '.content', '.main-content', '.post-content', 
                    '.entry-content', '#content', 'main', '.article-body', 
                    '.story-body', '.post-body', '.content-body'
                ]
                
                content = ""
                for selector in content_selectors:
                    content_elem = soup.select_one(selector)
                    if content_elem:
                        content = content_elem.get_text(separator=' ', strip=True)
                        break
                
                # Náº¿u khÃ´ng tÃ¬m tháº¥y, láº¥y tá»« body
                if not content:
                    body = soup.find('body')
                    if body:
                        content = body.get_text(separator=' ', strip=True)
                
                # LÃ m sáº¡ch vÃ  giá»›i háº¡n Ä‘á»™ dÃ i
                if content:
                    content = re.sub(r'\s+', ' ', content).strip()
                    return content[:max_length] + "..." if len(content) > max_length else content
            
        except Exception as e:
            print(f"âš ï¸ Lá»—i scrape {url}: {e}")
        
        return ""

    def start_interactive_search(self):
        """Báº¯t Ä‘áº§u phiÃªn tÃ¬m kiáº¿m tÆ°Æ¡ng tÃ¡c."""
        print("=" * 60)
        print("ğŸ” Há»† THá»NG TÃŒM KIáº¾M THÃ”NG MINH Vá»šI GEMINI AI")
        print("=" * 60)
        print("ğŸ’¡ Chá»‰ cáº§n nháº­p cÃ¢u há»i, há»‡ thá»‘ng sáº½ tá»± Ä‘á»™ng:")
        print("   â€¢ TÃ¬m kiáº¿m trÃªn Google + DuckDuckGo")
        print("   â€¢ Káº¿t há»£p vÃ  loáº¡i bá» trÃ¹ng láº·p")
        print("   â€¢ Sáº¯p xáº¿p káº¿t quáº£ theo Ä‘á»™ liÃªn quan vá»›i Reranker")
        print("   â€¢ Táº¡o cÃ¢u tráº£ lá»i thÃ´ng minh vá»›i Gemini AI")
        print("=" * 60)
        print("Nháº­p 'quit' Ä‘á»ƒ thoÃ¡t | 'history' Ä‘á»ƒ xem lá»‹ch sá»­ | 'help' Ä‘á»ƒ xem hÆ°á»›ng dáº«n")
        print("-" * 60)
        
        while True:
            try:
                query = input("\nğŸ’¬ Há»i gÃ¬ Ä‘Ã³: ").strip()
                
                if not query:
                    print("âš ï¸ Vui lÃ²ng nháº­p cÃ¢u há»i!")
                    continue
                
                if query.lower() in ['quit', 'exit', 'thoÃ¡t']:
                    print("ğŸ‘‹ Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng! Táº¡m biá»‡t!")
                    break
                
                elif query.lower() in ['history', 'lá»‹ch sá»­']:
                    self.show_search_history()
                    continue
                
                elif query.lower() in ['help', 'hÆ°á»›ng dáº«n']:
                    self.show_help()
                    continue
                
                # Thá»±c hiá»‡n tÃ¬m kiáº¿m vÃ  tráº£ lá»i
                self.perform_search(query)
                
                continue_choice = input("\nâ“ Tiáº¿p tá»¥c há»i? (Enter Ä‘á»ƒ tiáº¿p tá»¥c, 'q' Ä‘á»ƒ thoÃ¡t): ").strip()
                if continue_choice.lower() in ['q', 'quit', 'thoÃ¡t']:
                    print("ğŸ‘‹ Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng!")
                    break
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng! Táº¡m biá»‡t!")
                break
            except Exception as e:
                print(f"âŒ Lá»—i: {e}")
                print("ğŸ”„ Vui lÃ²ng thá»­ láº¡i...")

    def perform_search(self, query: str, total_results: int = 10, top_k: int = 5):
        """Thá»±c hiá»‡n tÃ¬m kiáº¿m káº¿t há»£p vá»›i query."""
        print(f"\nğŸ” Äang tÃ¬m kiáº¿m: '{query}'")
        print("â³ Vui lÃ²ng Ä‘á»£i...")
        
        # LÆ°u vÃ o lá»‹ch sá»­
        self.search_history.append({
            'query': query,
            'timestamp': self.get_current_time()
        })
        
        # TÃ¬m kiáº¿m káº¿t há»£p Google + DuckDuckGo
        combined_results = self.search_combined(query, total_results=total_results)
        
        if not combined_results:
            print("âŒ KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£ nÃ o! Vui lÃ²ng thá»­ láº¡i sau.")
            return
            
        print(f"âœ… Tá»•ng cá»™ng tÃ¬m tháº¥y {len(combined_results)} káº¿t quáº£ tá»« cáº£ Google vÃ  DuckDuckGo")
        
        # Rerank vá»›i hybrid method
        print("ğŸ”„ Äang sáº¯p xáº¿p láº¡i káº¿t quáº£ theo Ä‘á»™ liÃªn quan vá»›i Reranker...")
        ranked_results = self.reranker.rerank_hybrid(query, combined_results, top_k)
        
        # Táº¡o context tá»« káº¿t quáº£ Ä‘Ã£ rerank
        search_context = self._format_search_context(ranked_results)
        
        # Gá»i Gemini API Ä‘á»ƒ táº¡o cÃ¢u tráº£ lá»i
        print("ğŸ¤– Äang táº¡o cÃ¢u tráº£ lá»i vá»›i Gemini AI...")
        answer = self._generate_answer_with_gemini(query, search_context)
        
        # Hiá»ƒn thá»‹ káº¿t quáº£ cuá»‘i cÃ¹ng
        self.display_final_answer(query, answer, ranked_results)

    def _format_search_context(self, ranked_results: List[RankedResult]) -> str:
        """Format search context Ä‘á»ƒ gá»­i cho Gemini."""
        if not ranked_results:
            return "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan."
        
        context = "THÃ”NG TIN TÃŒM KIáº¾M (ÄÃƒ Sáº®P Xáº¾P THEO Äá»˜ LIÃŠN QUAN - GOOGLE + DUCKDUCKGO):\n\n"
        
        for i, ranked in enumerate(ranked_results, 1):
            result = ranked.original_result
            context += f"[Nguá»“n {i} - {result.source}] {result.title}\n"
            context += f"URL: {result.url}\n"
            context += f"MÃ´ táº£: {result.description}\n"
            
            if result.content:
                content = result.content[:800] + "..." if len(result.content) > 800 else result.content
                context += f"Ná»™i dung: {content}\n"
            
            context += f"Äiá»ƒm liÃªn quan: {ranked.combined_score:.2f}\n"
            context += "\n" + "-"*50 + "\n\n"
        
        return context

    def _generate_answer_with_gemini(self, query: str, search_context: str) -> str:
        """Gá»i Gemini API Ä‘á»ƒ táº¡o cÃ¢u tráº£ lá»i vá»›i time context."""
        base_prompt = f"""
Báº¡n lÃ  má»™t trá»£ lÃ½ AI thÃ´ng minh vÃ  há»¯u Ã­ch. HÃ£y tráº£ lá»i cÃ¢u há»i dá»±a trÃªn thÃ´ng tin tÃ¬m kiáº¿m Ä‘Æ°á»£c cung cáº¥p tá»« Google vÃ  DuckDuckGo.

CÃ‚U Há»I: {query}

{search_context}

HÆ¯á»šNG DáºªN TRáº¢ Lá»œI:
1. Tráº£ lá»i trá»±c tiáº¿p vÃ  Ä‘áº§y Ä‘á»§ cÃ¢u há»i
2. Sá»­ dá»¥ng thÃ´ng tin tá»« cÃ¡c nguá»“n Ä‘Ã¡ng tin cáº­y (Æ°u tiÃªn nguá»“n cÃ³ Ä‘iá»ƒm cao)
3. Tá»•ng há»£p thÃ´ng tin tá»« nhiá»u nguá»“n Google vÃ  DuckDuckGo Ä‘á»ƒ Ä‘Æ°a ra cÃ¢u tráº£ lá»i toÃ n diá»‡n
4. Äá» cáº­p nguá»“n thÃ´ng tin khi cáº§n thiáº¿t (vÃ­ dá»¥: "Theo nguá»“n 1..." hoáº·c "CÃ¡c nghiÃªn cá»©u cho tháº¥y...")
5. Náº¿u cÃ³ thÃ´ng tin mÃ¢u thuáº«n giá»¯a cÃ¡c nguá»“n, hÃ£y chá»‰ ra vÃ  Ä‘Æ°a ra quan Ä‘iá»ƒm cÃ¢n báº±ng
6. Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, rÃµ rÃ ng vÃ  dá»… hiá»ƒu
7. Cáº¥u trÃºc cÃ¢u tráº£ lá»i má»™t cÃ¡ch logic vÃ  cÃ³ tá»• chá»©c
8. Náº¿u khÃ´ng cÃ³ Ä‘á»§ thÃ´ng tin Ä‘á»ƒ tráº£ lá»i Ä‘áº§y Ä‘á»§, hÃ£y nÃ³i rÃµ Ä‘iá»u nÃ y

CÃ¢u tráº£ lá»i:
"""
        
        # ThÃªm time context náº¿u cáº§n
        enhanced_prompt = self.add_time_context_to_prompt(base_prompt, query)
        
        return self.gemini_api.generate_answer(enhanced_prompt)

    def display_final_answer(self, query: str, answer: str, ranked_results: List[RankedResult]):
        """Hiá»ƒn thá»‹ cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng."""
        print("\n" + "="*70)
        print("ğŸ¯ CÃ‚U TRáº¢ Lá»œI CHO CÃ‚U Há»I Cá»¦A Báº N")
        print("="*70)
        print(f"â“ CÃ¢u há»i: {query}")
        print("-"*70)
        print("ğŸ¤– Tráº£ lá»i tá»« Gemini AI:")
        print(answer)
        print("-"*70)
        
        # Hiá»ƒn thá»‹ nguá»“n tham kháº£o vá»›i thÃ´ng tin search engine
        print("ğŸ“š Nguá»“n tham kháº£o (Google + DuckDuckGo):")
        for i, ranked in enumerate(ranked_results, 1):
            result = ranked.original_result
            print(f"  {i}. [{result.source}] {result.title}")
            print(f"     {result.url}")
            print(f"     Äá»™ tin cáº­y: {ranked.combined_score:.1%}")
        
        print("="*70)

    def show_search_history(self):
        """Hiá»ƒn thá»‹ lá»‹ch sá»­ tÃ¬m kiáº¿m."""
        if not self.search_history:
            print("ğŸ“ ChÆ°a cÃ³ lá»‹ch sá»­ tÃ¬m kiáº¿m nÃ o!")
            return
        
        print("\n" + "="*50)
        print("ğŸ“š Lá»ŠCH Sá»¬ TÃŒM KIáº¾M")
        print("="*50)
        
        for i, item in enumerate(self.search_history[-10:], 1):
            print(f"{i}. '{item['query']}' - {item['timestamp']}")

    def show_help(self):
        """Hiá»ƒn thá»‹ hÆ°á»›ng dáº«n sá»­ dá»¥ng."""
        print("\n" + "="*50)
        print("ğŸ“– HÆ¯á»šNG DáºªN Sá»¬ Dá»¤NG")
        print("="*50)
        print("ğŸ”¹ Nháº­p cÃ¢u há»i báº¥t ká»³ Ä‘á»ƒ Ä‘Æ°á»£c tráº£ lá»i bá»Ÿi Gemini AI")
        print("ğŸ”¹ Nháº­p 'quit' hoáº·c 'thoÃ¡t' Ä‘á»ƒ thoÃ¡t")
        print("ğŸ”¹ Nháº­p 'history' hoáº·c 'lá»‹ch sá»­' Ä‘á»ƒ xem lá»‹ch sá»­")
        print("ğŸ”¹ Nháº­p 'help' hoáº·c 'hÆ°á»›ng dáº«n' Ä‘á»ƒ xem hÆ°á»›ng dáº«n nÃ y")
        print("\nâš™ï¸ Há»‡ thá»‘ng tá»± Ä‘á»™ng:")
        print("â€¢ TÃ¬m kiáº¿m káº¿t há»£p trÃªn Google + DuckDuckGo (má»—i engine 5 káº¿t quáº£)")
        print("â€¢ Loáº¡i bá» URL trÃ¹ng láº·p giá»¯a cÃ¡c search engine")
        print("â€¢ Scrape ná»™i dung tá»« cÃ¡c trang web")
        print("â€¢ Sáº¯p xáº¿p káº¿t quáº£ theo Ä‘á»™ liÃªn quan (Hybrid Reranking)")
        print("â€¢ Táº¡o cÃ¢u tráº£ lá»i thÃ´ng minh vá»›i Gemini AI")
        print("\nğŸ’¡ VÃ­ dá»¥ cÃ¢u há»i:")
        print("â€¢ 'Python lÃ  gÃ¬ vÃ  táº¡i sao nÃªn há»c?'")
        print("â€¢ 'CÃ¡ch há»c machine learning hiá»‡u quáº£'")
        print("â€¢ 'Xu hÆ°á»›ng cÃ´ng nghá»‡ AI 2024'")
        print("â€¢ 'Lá»£i Ã­ch cá»§a viá»‡c táº­p thá»ƒ dá»¥c'")
        print("\nâš ï¸ LÆ°u Ã½:")
        print("â€¢ Há»‡ thá»‘ng sá»­ dá»¥ng web scraping, cÃ³ thá»ƒ cháº­m hÆ¡n API")
        print("â€¢ TrÃ¡nh tÃ¬m kiáº¿m quÃ¡ nhiá»u láº§n liÃªn tiáº¿p Ä‘á»ƒ khÃ´ng bá»‹ cháº·n")
        print("â€¢ Káº¿t quáº£ Ä‘Æ°á»£c káº¿t há»£p tá»« cáº£ Google vÃ  DuckDuckGo")

    def get_current_time(self) -> str:
        """Láº¥y thá»i gian hiá»‡n táº¡i."""
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    def search_with_time_awareness(self, query: str, total_results: int = 10, top_k: int = 5):
        """TÃ¬m kiáº¿m vá»›i kháº£ nÄƒng nháº­n biáº¿t thá»i gian."""
        print(f"\nğŸ” Äang phÃ¢n tÃ­ch cÃ¢u há»i: '{query}'")
        
        # Kiá»ƒm tra náº¿u cÃ¢u há»i liÃªn quan Ä‘áº¿n thá»i gian
        if self.detect_time_sensitive_query(query):
            print("â° PhÃ¡t hiá»‡n cÃ¢u há»i liÃªn quan thá»i gian - sáº½ Æ°u tiÃªn thÃ´ng tin má»›i nháº¥t")
            # ThÃªm tá»« khÃ³a Ä‘á»ƒ tÃ¬m thÃ´ng tin má»›i
            enhanced_query = f"{query} 2024 2025 latest current"
        else:
            enhanced_query = query
        
        # Thá»±c hiá»‡n tÃ¬m kiáº¿m
        self.perform_search(enhanced_query, total_results, top_k)